import streamlit as st
import google.generativeai as genai
from google.api_core import retry

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© ---
st.set_page_config(page_title="Gemini Expert Engine", layout="wide")

# --- Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¹Ø±Ø¨ÙŠ ---
st.markdown("""
<style>
    .stTextArea textarea {direction: rtl; font-size: 16px; font-family: 'Courier New', monospace;}
    div[data-testid="stChatMessage"] {direction: rtl; text-align: right; background-color: #262730; border: 1px solid #444;}
    .stStatus {direction: rtl;}
</style>
""", unsafe_allow_html=True)

# --- 1. Ø§Ù„Ù…ØµØ§Ø¯Ù‚Ø© ---
api_keys = [v for k, v in st.secrets.items() if k.startswith("KEY_")]
if not api_keys:
    st.error("âŒ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ù…ÙÙ‚ÙˆØ¯Ø© ÙÙŠ Secrets")
    st.stop()

genai.configure(api_key=api_keys[0])

# --- 2. Ø§Ù„Ø¯Ø³ØªÙˆØ± (System Instructions) - Ø³Ø± Ø§Ù„Ù‚ÙˆØ© ---
# Ù‡Ø°Ø§ Ù…Ø§ ÙŠØ¬Ø¹Ù„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø°ÙƒÙŠØ§Ù‹ Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù† Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø£Ù‚Ù„
SYSTEM_PROMPT = """
Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø¨Ø±Ù…Ø¬ÙŠØ§Øª (Senior Solutions Architect).
- Ù„ØºØªÙƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ØªÙ‚Ù†ÙŠØ©.
- Ø£Ø³Ù„ÙˆØ¨Ùƒ: Ù…Ø¨Ø§Ø´Ø±ØŒ ØµØ§Ø±Ù…ØŒ ÙˆÙŠØ­Ù„Ù„ Ø¬Ø°ÙˆØ± Ø§Ù„Ù…Ø´ÙƒÙ„Ø© (Root Cause).
- Ù…Ù…Ù†ÙˆØ¹ Ø§Ù„Ø§Ø¹ØªØ°Ø§Ø± Ø£Ùˆ Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„Ø²Ø§Ø¦Ø¯.
- Ø¹Ù†Ø¯ Ø·Ù„Ø¨ ÙƒÙˆØ¯ØŒ Ø§ÙƒØªØ¨ Ø§Ù„ÙƒÙˆØ¯ ÙƒØ§Ù…Ù„Ø§Ù‹ (Production Ready).
"""

# --- 3. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª ---
# Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¨Ø±Ùˆ Ø£ÙˆÙ„Ø§Ù‹ØŒ Ø«Ù… Ø§Ù„ÙÙ„Ø§Ø´ ÙƒØ®Ø·Ø© Ø¨Ø¯ÙŠÙ„Ø©
try_models = ["gemini-1.5-pro", "gemini-1.5-flash"]
active_model = None

# ÙØ­Øµ Ø³Ø±ÙŠØ¹ Ø£ÙŠÙ‡Ù…Ø§ ÙŠØ¹Ù…Ù„
with st.sidebar:
    st.header("âš™ï¸ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø±Ùƒ")
    status_box = st.status("Ø¬Ø§Ø±ÙŠ ÙØ­Øµ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª...", expanded=True)
    
    for m in try_models:
        try:
            status_box.write(f"ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ {m}...")
            test_model = genai.GenerativeModel(m)
            # ØªØ¬Ø±Ø¨Ø© ÙˆÙ‡Ù…ÙŠØ© Ø³Ø±ÙŠØ¹Ø© Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„ÙƒÙˆØªØ§
            test_model.generate_content("test")
            active_model = m
            status_box.update(label=f"âœ… ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù†Ø¬Ø§Ø­: {m}", state="complete", expanded=False)
            break # ÙˆØ¬Ø¯Ù†Ø§ ÙˆØ§Ø­Ø¯Ø§Ù‹ ÙŠØ¹Ù…Ù„ØŒ Ù†ØªÙˆÙ‚Ù
        except Exception as e:
            if "429" in str(e) or "Quota" in str(e):
                status_box.write(f"âŒ {m}: Ø§Ù„Ø±ØµÙŠØ¯ 0 (Ù…Ø­Ø¸ÙˆØ± ÙÙŠ Ù…Ù†Ø·Ù‚ØªÙƒ)")
            else:
                status_box.write(f"âŒ {m}: Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹")
    
    if not active_model:
        status_box.update(label="â›” ÙƒÙ„ Ø§Ù„Ù…Ù†Ø§ÙØ° Ù…ØºÙ„Ù‚Ø©!", state="error")
        st.error("Ù„Ù„Ø£Ø³ÙØŒ Ø¬ÙˆØ¬Ù„ ØªØºÙ„Ù‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠØ© Ù„Ù‡Ø°Ø§ Ø§Ù„Ù…ÙØªØ§Ø­. Ø§Ù„Ø­Ù„ Ø§Ù„ÙˆØ­ÙŠØ¯ Ù‡Ùˆ Ø­Ø³Ø§Ø¨ Ø¬Ø¯ÙŠØ¯.")
        st.stop()

    st.success(f"Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ù†Ø´Ø·: `{active_model}`")
    if active_model == "gemini-1.5-flash":
        st.warning("âš ï¸ ØªÙ†Ø¨ÙŠÙ‡: Ù†Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Flash Ù„Ø£Ù† Pro Ù…Ø­Ø¸ÙˆØ±ØŒ Ù„ÙƒÙ† ØªÙ… ØªÙØ¹ÙŠÙ„ 'ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø®Ø¨Ø±Ø§Ø¡' Ù„Ø±ÙØ¹ Ø§Ù„ÙƒÙØ§Ø¡Ø©.")

# --- 4. ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø­Ø±Ùƒ ---
model = genai.GenerativeModel(
    active_model,
    system_instruction=SYSTEM_PROMPT
)

st.title("ğŸš€ ØºØ±ÙØ© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª (Ø¨Ø¯ÙˆÙ† VPN)")

if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("Ø£Ø¯Ø®Ù„ Ø§Ù„ÙƒÙˆØ¯ Ø£Ùˆ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ù‡Ù†Ø§..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner('Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©...'):
            try:
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                history = [{"role": m["role"].replace("assistant", "model"), "parts": [m["content"]]} 
                           for m in st.session_state.messages[:-1]]
                
                chat = model.start_chat(history=history)
                response = chat.send_message(prompt)
                
                st.markdown(response.text)
                st.session_state.messages.append({"role": "assistant", "content": response.text})
                
            except Exception as e:
                st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£: {e}")
